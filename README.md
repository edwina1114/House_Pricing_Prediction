# ğŸ¡ House Prices Prediction Project

## ğŸ“Œ Overview

This project aims to predict house prices using the Ames Housing dataset. It includes comprehensive steps from **exploratory data analysis (EDA)** to **model selection** and **feature optimization**.

ğŸ“‚ Dataset Source:  
[Kaggle - House Prices: Advanced Regression Techniques](https://www.kaggle.com/competitions/home-data-for-ml-course)

The process is documented in two Jupyter Notebooks:

- [01_eda_insights.ipynb](notebooks/01_eda_insights.ipynb): EDA and feature understanding  
- [02_complete_modeling.ipynb](notebooks/02_complete_modeling.ipynb): Modeling, comparison, and final model refinement

---
## ğŸš€ Live Demo
ğŸ–¥ï¸ Explore the interactive dashboard and in-depth insights:
[ğŸ”— Streamlit Live Demo](https://edwina1114-house-pricing-prediction-apphome-3qkfna.streamlit.app/)

The Streamlit app includes:

- ğŸ¯ EDA summaries and visualizations

- ğŸ“ˆ Model performance comparisons

- ğŸ§  Feature selection insights

- ğŸ“Š Dynamic charts and metrics
  
---

## ğŸ” Exploratory Data Analysis Highlights

Key insights from [01_eda_insights.ipynb](notebooks/01_eda_insights.ipynb):

- ğŸ”„ **Target Transformation**: `SalePrice` was log-transformed to reduce right skew and improve model fit  
- ğŸ§± **Missing Value Handling**: Applied customized strategies based on feature semantics (e.g., quality vs presence)  
- ğŸ§® **Categorical Cardinality**: High-cardinality features (e.g., `Neighborhood`) were frequency-encoded; others used one-hot encoding  
- ğŸ“Š **Feature Correlation**: `OverallQual`, `GrLivArea`, and `GarageCars` showed strongest positive correlations with price  

> ğŸ“ Visualizations available in [images/EDA/](images/EDA/) include:  
> - [Top Correlated Features](images/EDA/correlation_with_saleprice.png) 
> - [Categorical Features Cardinality](images/EDA/cardinality_of_categorical_features.png)
> - [Missing Value Box/Count Plots](images/EDA/box_count_missing_features.png)  
> - [Log-Transformed Target Distribution](images/EDA/target_distribution_log.png)

---

## ğŸ§ª Modeling Pipeline

Key steps and insights from [02_complete_modeling.ipynb](notebooks/02_complete_modeling.ipynb):

- ğŸ§¹ **Data Preprocessing**
  - Built custom transformers based on EDA findings:
    - Ordinal + Binary Indicators for quality-related features (e.g., `PoolQC`, `FireplaceQu`)
    - Binary indicators for sparse categorical features (e.g., `Alley`, `Fence`)
    - Frequency encoding for high-cardinality features (`Neighborhood`, `Exterior1st`, `Exterior2nd`)
  - Combined all pipelines using `ColumnTransformer`

- ğŸ§ª **Modeling & Evaluation**
  - Compared five models: `Linear`, `Ridge`, `Lasso`, `Random Forest`, and `XGBoost`
  - Evaluation metrics included:
    - **MAE** (Mean Absolute Error)
    - **RÂ² Score**
    - **Overfitting Gap (%)** = Train RÂ² âˆ’ Valid RÂ²

> ğŸ“Š Visual summary of model comparison:  
> ![Model Comparison](images/Model/model_comparison_results.png)

---

### âœ… Final Model Selection: Lasso Regression

- Balanced performance: low MAE (~17,139), high RÂ² (~0.912), and lowest overfitting gap (~12%)
- Built-in **feature selection** improved generalization and simplified model structure

> ğŸ“‰ Residual & prediction comparison across models:  
> ![Residuals vs Predictions](images/Model/residuals_vs_predicted_comparison_across_models.png)

---

### ğŸ” Feature Selection with Lasso

- Retained **76 out of 220 features (34.5%)** without loss in performance  
- Achieved a **2.9x compression ratio** while maintaining MAE and RÂ²

> ğŸ§  Top 20 influential features from Lasso:  
> ![Lasso Top 20 Features](images/Model/top20_features_from_lasso.png)

---

### ğŸ“ˆ Performance Before vs After Feature Selection

| Metric | Before | After |
|--------|--------|-------|
| MAE    | 17,139 | 17,139 |
| RÂ²     | 0.9120 | 0.9120 |
| Retained Features | 220 | 76 |

> ğŸ“Š Comparison of model performance before and after feature selection:  
> ![Before vs After](images/Model/model_performance_before_after_feature_selection.png)

---

## ğŸ› ï¸ Tech Stack

- **Programming & Data Manipulation**: `Python`, `Pandas`, `NumPy`
- **Modeling & Evaluation**: `Scikit-learn` (`Pipeline`, `GridSearchCV`, `LassoCV`, `RidgeCV`, `SelectFromModel`, `TransformedTargetRegressor`), `XGBoost`
- **Visualization**: `Matplotlib`, `Seaborn`, `Plotly (express & graph_objects)`
- **Environment**: `Jupyter Notebook`


---

## ğŸ“ File Structure
```
HOUSE_PRICING_PREDICTION/
â”œâ”€â”€ app/                       # Streamlit app and supporting utils
â”‚   â”œâ”€â”€ pages/                 # Streamlit tab pages
â”‚   â”œâ”€â”€ utils/                 # Custom Functions
â”‚   â”œâ”€â”€ data/                  # Intermediate CSVs for visualization
â”‚   â””â”€â”€ Home.py                # Main dashboard entry
â”œâ”€â”€ data/ # Raw training and test datasets
â”‚ â”œâ”€â”€ train.csv
â”‚ â””â”€â”€ test.csv
â”œâ”€â”€ images/ # All result plots and visualizations
â”‚ â”œâ”€â”€ EDA/ # Visualizations from exploratory analysis
â”‚ â””â”€â”€ Model/ # Model performance plots
â”œâ”€â”€ notebooks/ 
â”‚ â”œâ”€â”€ 01_eda_insights.ipynb # EDA and feature analysis
â”‚ â””â”€â”€ 02_complete_modeling.ipynb # Modeling and evaluation pipeline
â””â”€â”€ README.md # Project overview and documentation
```

---

## ğŸš€ Streamlit - How to Run Locally

### â–¶ï¸ Option 1: Pip (recommended for Streamlit Cloud)
```bash
pip install -r requirements.txt
streamlit run app/Home.py
```

### â–¶ï¸ Option 2: With conda
```bash
conda env create -f environment.yml
conda activate streamlit_practice
streamlit run app/Home.py
```
