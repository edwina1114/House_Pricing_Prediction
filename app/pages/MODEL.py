import streamlit as st
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.access import get_image
from utils.model_plot import (
    create_comparison_plots,
    plot_before_after_performance,
    plot_top_features,
)

st.header("üß™ Modeling")

tab1, tab2, tab3 = st.tabs(
    [
        "‚öôÔ∏è Modeling Pipeline",
        "üìà Model Comparison",
        "üß† Feature Selection Insights",
    ]
)

with tab1:
    st.subheader("‚öôÔ∏è Modeling Pipeline")

    st.markdown(
        """

        This pipeline processes different feature groups using customized transformers:

        1. **Numerical Features**  
           ‚û§ Imputed using **mean** ‚Üí Scaled with **StandardScaler**

        2. **Categorical Features (Low Cardinality)**  
           ‚û§ Imputed using **most frequent** ‚Üí Encoded with **OneHotEncoder**

        3. **Quality-Based Features** (`PoolQC`, `FireplaceQu`, etc.)  
           ‚û§ Imputed with `"None"`  
           ‚û§ Transformed into **ordinal values** (e.g., None < TA < Gd < Ex)  
           ‚û§ **Binary presence indicators** also added

        4. **Presence-Only Features** (`Alley`, `Fence`, `MiscFeature`, etc.)  
           ‚û§ Imputed with `"None"`  
           ‚û§ Encoded only as **binary indicators** (e.g., `HasAlley`)

        5. **High-Cardinality Categorical Features** (`Neighborhood`, `Exterior1st`, `Exterior2nd`)  
           ‚û§ Transformed using **Frequency Encoding** (based on relative frequency)

        """
    )

    st.image(
        get_image("MODEL", "pipeline.png"),
        use_container_width=True,
    )

with tab2:
    st.subheader("üìà Model Overview")
    st.markdown(
        """
        - **Five models were evaluated with 5-Fold Cross Validation:**  
          Linear Regression, Ridge, Lasso, Random Forest, XGBoost

        - **Evaluation Metrics:**  
          - MAE (Mean Absolute Error)  
          - R¬≤ (Coefficient of Determination)  
          - Overfitting Gap (Train vs Validation R¬≤)

        - ‚úÖ **Lasso** showed the best trade-off between accuracy and generalization.
        """
    )

    st.plotly_chart(create_comparison_plots(), use_container_width=True)

    st.subheader("üìä Summary & Insights")
    st.markdown(
        """
        #### üîç Model Selection: Balancing Accuracy and Generalization
        - **Linear Regression** had top accuracy (MAE: 16,333; R¬≤: 0.920) but poor generalization with a **36% overfitting gap**.
        - A **weighted metric = MAE √ó (1 + Overfitting Gap%)** was used for fair comparison.

        #### ‚úÖ Best Overall: Lasso Regression
        - Strong MAE and **lowest overfitting gap (~12%)**.
        - Includes **automatic feature selection** for simplicity and robustness.

        #### ‚úÖ Linear > Tree-Based Models
        - **Linear, Ridge, Lasso** outperformed **Random Forest, XGBoost**
        - Implies data has **linear tendencies** and small size limits tree models (~1,460 rows)

        #### ‚ö†Ô∏è Weakness in High-Priced Homes
        - All models had **larger residuals on expensive homes**
        - **Future work:** Stratified modeling or feature engineering for luxury homes
        """
    )

    st.subheader("üîç Residuals vs Predicted Values")
    st.image(get_image("MODEL", "residuals_vs_predicted_comparison_across_models.png"))


with tab3:
    st.subheader("üß† Lasso Feature Selection Summary")
    st.markdown(
        """
        | Metric | Value |
        |--------|-------|
        | Final Model | Lasso Regression |
        | Feature Selector | Lasso + SelectFromModel |
        | Features Retained | 76 / 220 (34.5%) |
        | R¬≤ (Validation) | 0.9120 |
        | MAE (Validation) | 17,139 |
        | Performance Retention | 100% |
        | Compression Ratio | 2.9√ó |
        """
    )

    st.plotly_chart(plot_before_after_performance(), use_container_width=True)

    st.subheader("üß† Top 20 Features from Lasso")
    st.plotly_chart(plot_top_features(), use_container_width=True)

    st.markdown(
        """
        - `RoofMatl_ClyTile` has the strongest negative impact on price.  
          ‚û§ May reflect outdated design or poor neighborhood quality.

        - `GrLivArea` and `OverallQual` are top positive predictors.  
          ‚û§ Larger, higher-quality homes command better prices.

        - Most numerical features (e.g., `GarageCars`, `YearBuilt`) show positive influence.  
          ‚û§ Suggests that **size and recency** are key price drivers.
        """
    )
